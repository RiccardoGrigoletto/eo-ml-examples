{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Classification Step 3d: Model Training with Maggy - Hyperparameter Optimization\n",
    "The following code includes demonstration for:\n",
    "- get data from ``feature store``\n",
    "- training with ``TFRecord`` on a single GPU\n",
    "- hyperparameter optimization with ``maggy``\n",
    "\n",
    "This notebook is tested with the following ``configuration`` from hopsworks.\n",
    "<div>\n",
    "<img src=\"fig/step3d_3e_jupyter_config.png\" width=\"900\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>252</td><td>application_1619040920875_0285</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hopsworks-2.novalocal:8088/proxy/application_1619040920875_0285/\">Link</a></td><td><a target=\"_blank\" href=\"http://hopsworks-3.novalocal:8042/node/containerlogs/container_e05_1619040920875_0285_01_000001/ExtremeEarth__tianzew0\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "-----------------------------------------------\n",
      "This notebook is tested with:\n",
      "  - TensorFlow 2.4.1.\n",
      "  - Hopsworks 2.1.0.1.\n",
      "  - Maggy 0.5.0.\n",
      "  - Spark 2.4.3.2."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import hops\n",
    "from hops import tensorboard\n",
    "from hops import model as hopsworks_model\n",
    "import hsfs\n",
    "import maggy\n",
    "import maggy.version\n",
    "\n",
    "# SparkSession available as 'spark'\n",
    "print(\n",
    "    f\"-----------------------------------------------\\n\" \\\n",
    "    f\"This notebook is tested with:\\n\" \\\n",
    "    f\"  - TensorFlow {tf.__version__}.\\n\" \\\n",
    "    f\"  - Hopsworks {hops.__version__}.\\n\" \\\n",
    "    f\"  - Maggy {maggy.version.__version__}.\\n\" \\\n",
    "    f\"  - Spark {spark.version}.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(kernel, pool, dropout, input_shape):\n",
    "    \"\"\"Returns a CNN model for image classification.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_shape(tuple): input shape of the CNN model.\n",
    "    \n",
    "    Returns:\n",
    "    - a TensorFlow keras model that is not compiled yet.\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    #Conv Layer 1\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size=(kernel, kernel), activation='relu', input_shape=input_shape))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(pool, pool), strides=(2,2)))\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "\n",
    "    #Conv Layer 2\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size=(kernel, kernel), activation='relu' ))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(pool, pool), strides=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "    \n",
    "    #Conv Layer 3\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 4\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Flatten the data for upcoming dense layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    #Dense Layers\n",
    "    model.add(tf.keras.layers.Dense(512))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Dense Layer 2\n",
    "    model.add(tf.keras.layers.Dense(256))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    #Sigmoid Layer\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maggy.callbacks import KerasBatchEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(kernel, pool, dropout, reporter):\n",
    "    \"\"\"\"Wrapper function for the experiment.\n",
    "    \n",
    "    Parameters:\n",
    "    - learning_rate: learning rate of the optimizer during training.\n",
    "    \n",
    "    Returns:\n",
    "    - metrics: training summary.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # ---------------- Initialization ----------------\n",
    "    # Establish a connection with the Hopsworks feature store\n",
    "    #     engine='training' is needed so that the executors in Spark can connect to feature store\n",
    "    connection = hsfs.connection(engine='training') \n",
    "    # Get the feature store handle for the project's feature store\n",
    "    fs = connection.get_feature_store()\n",
    "    \n",
    "    # Clear session info\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    # Set up visible GPU\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "          tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "      except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "    \n",
    "    # ---------------- Initialization ----------------\n",
    "    \n",
    "    # ---------------- Hyperparameters ----------------\n",
    "    # Number of epochs to training\n",
    "    EPOCHS = 10\n",
    "    # Training batch size\n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    # Evaluation batch size\n",
    "    EVAL_BATCH_SIZE = 1\n",
    "    # Shuffle buffer size for TensorFlow dataset\n",
    "    SHUFFLE_BUFFER_SIZE = 10000\n",
    "    # learning rate of the optimizer during training\n",
    "    LEARNING_RATE = 0.001\n",
    "    # input_shape of the model\n",
    "    INPUT_SHAPE= (75, 75, 3)\n",
    "    # Name of the training dataset in feature store\n",
    "    TRAIN_FS_NAME = 'train_tfrecords_iceberg_classification_dataset'\n",
    "    # Name of the test dataset in feature sotre\n",
    "    TEST_FS_NAME = 'test_tfrecords_iceberg_classification_dataset'\n",
    "    \n",
    "    # ---------------- Hyperparameters ----------------\n",
    "    \n",
    "    # ---------------- Training Process ----------------\n",
    "    # use this strategy to test your code before switching to other strategies which actually distributes to multiple devices/machines.\n",
    "    strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\n",
    "\n",
    "    # construct model under distribution strategy scope\n",
    "    with strategy.scope(): \n",
    "        model = create_model(kernel, pool, dropout, INPUT_SHAPE)\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE), loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    \n",
    "    callbacks = [KerasBatchEnd(reporter, metric='acc')]\n",
    "\n",
    "    \n",
    "    def decode(sample):\n",
    "        \"\"\"Decode each training sample.\n",
    "        \n",
    "        This funtionc decode each sample and return it in a format that is ready for training.\n",
    "        \n",
    "        Parameters:\n",
    "        - sample: raw features of a data sample stored in a dictionary-like object\n",
    "        \n",
    "        Returns:\n",
    "        - x: 'band_1', 'band_2', and 'band_avg' will be reshaped and stacked\n",
    "             and form the input of the model\n",
    "        - y: 'is_iceberg' will be the output of the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        name_list = ['band_1', 'band_2', 'band_avg', 'is_iceberg']\n",
    "        x = tf.stack([sample[name_list[0]], sample[name_list[1]], sample[name_list[2]]], axis=1)\n",
    "        x = tf.reshape(x, [75, 75, 3])\n",
    "        y = [tf.cast(sample[name_list[3]], tf.float32)]\n",
    "        return x,y\n",
    "    \n",
    "    # Training dataset in TFRecord format\n",
    "    train_ds = fs.get_training_dataset(name=TRAIN_FS_NAME).tf_data(target_name='is_iceberg')\n",
    "    train_ds = train_ds.tf_record_dataset(process=False, batch_size=TRAIN_BATCH_SIZE, num_epochs=EPOCHS)\n",
    "    train_ds_processed = train_ds.map(decode).shuffle(SHUFFLE_BUFFER_SIZE).repeat(EPOCHS).cache().batch(TRAIN_BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    # Evaluation dataset in TFRecord format\n",
    "    eval_ds = fs.get_training_dataset(name=TEST_FS_NAME).tf_data(target_name='is_iceberg')\n",
    "    eval_ds = eval_ds.tf_record_dataset(process=False, batch_size=EVAL_BATCH_SIZE, num_epochs=EPOCHS)\n",
    "    eval_ds_processed = eval_ds.map(decode).shuffle(SHUFFLE_BUFFER_SIZE).repeat(EPOCHS).cache().batch(EVAL_BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    # Start training the model.\n",
    "    history = model.fit(\n",
    "        train_ds_processed,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1,\n",
    "        validation_data=eval_ds_processed,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # 'metrics' is the return value of this function;\n",
    "    #     The values in 'metrics' will be printed to the notebook cell that launch the experiment\n",
    "    metrics = {\n",
    "        'train_loss': history.history['loss'][-1],\n",
    "        'train_accuracy': history.history['accuracy'][-1],\n",
    "        'val_loss': history.history['val_loss'][-1],\n",
    "        'val_accuracy': history.history['val_accuracy'][-1],\n",
    "    } \n",
    "\n",
    "    # ---------------- Training Process ----------------\n",
    "    \n",
    "#     # ---------------- Save and Export ----------------\n",
    "#     # Export model as savedModel\n",
    "# #     export_path = tensorboard.logdir() + '/SavedModel'\n",
    "\n",
    "#     tf.keras.models.save_model(\n",
    "#         model,\n",
    "#         export_path,\n",
    "#         overwrite=True,\n",
    "#         include_optimizer=True,\n",
    "#         save_format=None,\n",
    "#         signatures=None,\n",
    "#         options=None\n",
    "#     )\n",
    "    \n",
    "#     # 'hopsworks_model' is the moudle provided by hopsworks for exporting models\n",
    "#     # 'hopsworks_model' is a different name of 'hops.model' to avoid name clashes\n",
    "#     hopsworks_model.export(export_path, 'ship_iceberg_classifier', metrics=metrics)\n",
    "#     # ---------------- Save and Export ----------------\n",
    "    \n",
    "    return metrics['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the search space for hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter added: kernel\n",
      "Hyperparameter added: pool\n",
      "Hyperparameter added: dropout"
     ]
    }
   ],
   "source": [
    "from maggy import Searchspace\n",
    "\n",
    "# The searchspace can be instantiated with parameters\n",
    "sp = Searchspace()\n",
    "\n",
    "# Or additional parameters can be added one by one\n",
    "sp.add('kernel', ('INTEGER', [3, 4]))\n",
    "sp.add('pool', ('INTEGER', [2, 3]))\n",
    "sp.add('dropout', ('DOUBLE', [0.10, 0.50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: deepspeed and/or fairscale import failed. DeepSpeed backend and zero_lvl 3\n",
      "          won't be available"
     ]
    }
   ],
   "source": [
    "from maggy import experiment\n",
    "from maggy.experiment_config import OptimizationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OptimizationConfig(\n",
    "                            num_trials=10,\n",
    "                            optimizer='randomsearch',\n",
    "                            searchspace=sp,\n",
    "                            direction='max',\n",
    "                            es_interval=1,\n",
    "                            es_min=2,\n",
    "                            hb_interval=5,\n",
    "                            name='Iceberg_Classification_Maggy'\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089c4836575b45ee98a23bb86390445d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Maggy experiment', max=10.0, style=ProgressStyle(descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Connected. Call `.close()` to terminate connection gracefully.\n",
      "0: Physical devices cannot be modified after being initialized\n",
      "0: \n",
      "0: \n",
      "0: Epoch 1/10\n",
      "0: \n",
      "0: \n",
      "1: Connected. Call `.close()` to terminate connection gracefully.\n",
      "1: Physical devices cannot be modified after being initialized\n",
      "1: \n",
      "1: \n",
      "1: Epoch 1/10\n",
      "1: \n",
      "1: \n",
      "0: \n",
      "0: \n",
      "1: \n",
      "1: \n",
      "0: Epoch 2/10\n",
      "1: Epoch 2/10\n",
      "0: Epoch 3/10\n",
      "0: Epoch 4/10\n",
      "1: Epoch 3/10\n",
      "0: Epoch 5/10\n",
      "1: Epoch 4/10\n",
      "0: Epoch 6/10\n",
      "1: Epoch 5/10\n",
      "0: Epoch 7/10\n",
      "1: Epoch 6/10\n",
      "0: Epoch 8/10\n",
      "0: Epoch 9/10\n",
      "1: Epoch 7/10\n",
      "0: Epoch 10/10\n",
      "1: Epoch 8/10\n",
      "0: Connected. Call `.close()` to terminate connection gracefully.\n",
      "0: Physical devices cannot be modified after being initialized\n",
      "0: Epoch 1/10\n",
      "1: Epoch 9/10\n",
      "0: Epoch 2/10\n",
      "1: Epoch 10/10\n",
      "0: Epoch 3/10\n",
      "1: Connected. Call `.close()` to terminate connection gracefully.\n",
      "1: Physical devices cannot be modified after being initialized\n",
      "1: Epoch 1/10\n",
      "0: Epoch 4/10\n",
      "0: Epoch 5/10\n",
      "1: Epoch 2/10\n",
      "0: Epoch 6/10\n",
      "1: Epoch 3/10\n",
      "0: Epoch 7/10\n",
      "1: Epoch 4/10\n",
      "0: Epoch 8/10\n",
      "0: Epoch 9/10\n",
      "1: Epoch 5/10\n",
      "0: Epoch 10/10\n",
      "1: Epoch 6/10\n",
      "0: Connected. Call `.close()` to terminate connection gracefully.\n",
      "0: Physical devices cannot be modified after being initialized\n",
      "0: Epoch 1/10\n",
      "1: Epoch 7/10\n",
      "0: Epoch 2/10\n",
      "1: Epoch 8/10\n",
      "0: Epoch 3/10\n",
      "1: Epoch 9/10\n",
      "0: Epoch 4/10\n",
      "0: Epoch 5/10\n",
      "1: Epoch 10/10\n",
      "0: Epoch 6/10\n",
      "1: Connected. Call `.close()` to terminate connection gracefully.\n",
      "1: Physical devices cannot be modified after being initialized\n",
      "1: Epoch 1/10\n",
      "0: Epoch 7/10\n",
      "0: Epoch 8/10\n",
      "1: Epoch 2/10\n",
      "0: Epoch 9/10\n",
      "1: Epoch 3/10\n",
      "0: Epoch 10/10\n",
      "1: Epoch 4/10\n",
      "0: Connected. Call `.close()` to terminate connection gracefully.\n",
      "0: Physical devices cannot be modified after being initialized\n",
      "0: Epoch 1/10\n",
      "1: Epoch 5/10\n",
      "0: Epoch 2/10\n",
      "1: Epoch 6/10\n",
      "0: Epoch 3/10\n",
      "1: Epoch 7/10\n",
      "0: Epoch 4/10\n",
      "1: Epoch 8/10\n",
      "0: Epoch 5/10\n",
      "0: Epoch 6/10\n",
      "1: Epoch 9/10\n",
      "0: Epoch 7/10\n",
      "1: Epoch 10/10\n",
      "0: Epoch 8/10\n",
      "1: Connected. Call `.close()` to terminate connection gracefully.\n",
      "1: Physical devices cannot be modified after being initialized\n",
      "1: Epoch 1/10\n",
      "0: Epoch 9/10\n",
      "0: Epoch 10/10\n",
      "1: Epoch 2/10\n",
      "0: Connected. Call `.close()` to terminate connection gracefully.\n",
      "0: Physical devices cannot be modified after being initialized\n",
      "0: Epoch 1/10\n",
      "1: Epoch 3/10\n",
      "0: Epoch 2/10\n",
      "1: Epoch 4/10\n",
      "0: Epoch 3/10\n",
      "1: Epoch 5/10\n",
      "0: Epoch 4/10\n",
      "1: Epoch 6/10\n",
      "0: Epoch 5/10\n",
      "0: Epoch 6/10\n",
      "1: Epoch 7/10\n",
      "0: Epoch 7/10\n",
      "1: Epoch 8/10\n",
      "0: Epoch 8/10\n",
      "1: Epoch 9/10\n",
      "0: Epoch 9/10\n",
      "1: Epoch 10/10\n",
      "0: Epoch 10/10\n",
      "0: Connected. Call `.close()` to terminate connection gracefully.\n",
      "0: Physical devices cannot be modified after being initialized\n",
      "0: Epoch 1/10\n",
      "0: Epoch 2/10\n",
      "0: Epoch 3/10\n",
      "0: Epoch 4/10\n",
      "0: Epoch 5/10\n",
      "0: Epoch 6/10\n",
      "0: Epoch 7/10\n",
      "0: Epoch 8/10\n",
      "0: Epoch 9/10\n",
      "0: Epoch 10/10\n",
      "You are running Maggy on Hopsworks.\n",
      "\n",
      "------ RandomSearch Results ------ direction(max) \n",
      "BEST combination {\"kernel\": 3, \"pool\": 2, \"dropout\": 0.16684081625495453} -- metric 0.4523880183696747\n",
      "WORST combination {\"kernel\": 3, \"pool\": 2, \"dropout\": 0.432150305724333} -- metric 0.24749210476875305\n",
      "AVERAGE metric -- 0.32687304317951205\n",
      "EARLY STOPPED Trials -- 0\n",
      "Total job time 0 hours, 20 minutes, 21 seconds\n",
      "\n",
      "Finished experiment.\n"
     ]
    }
   ],
   "source": [
    "result = experiment.lagom(train_fn=train_fn, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Step 3d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}