{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Classification Step 2: Create Feature Groups and Train/Eval datasets\n",
    "This notebook will perform the following operations:\n",
    "- Read the pre-processed data from a HopsFS dataset into a PySpark dataframe \n",
    "- Create and Feature Group \"iceberg\"\n",
    "- Create a training and test dataset with the Feature Store API\n",
    "\n",
    "This notebook is tested with the following ``configuration`` from hopsworks.\n",
    "<div>\n",
    "<img src=\"fig/step2_jupyter_config.png\" width=\"900\" align=\"center\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>62</td><td>application_1623657485747_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://maggydemo22-master.internal.cloudapp.net:8089/proxy/application_1623657485747_0002/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://maggydemo22-worker-1.internal.cloudapp.net:8044/node/containerlogs/container_e10_1623657485747_0002_01_000001/demo_ml_meb10180__meb10180\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "-----------------------------------------------\n",
      "This notebook is tested with:\n",
      "  - Hopsworks 2.2.0\n",
      "  - Spark 2.4.3.2."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hops import hdfs\n",
    "from hops import pandas_helper as pd\n",
    "import hsfs\n",
    "from hsfs.rule import Rule\n",
    "\n",
    "# SparkSession available as 'spark'\n",
    "print(\n",
    "    f\"-----------------------------------------------\\n\" \\\n",
    "    f\"This notebook is tested with:\\n\" \\\n",
    "    f\"  - Hopsworks 2.2.0\\n\" \\\n",
    "    f\"  - Spark {spark.version}.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define relevant paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds_path: hdfs://rpc.namenode.service.consul:8020/Projects/demo_ml_meb10180/eodata/train.json\n",
      "train_preprocessed_all_ds_path: hdfs://rpc.namenode.service.consul:8020/Projects/demo_ml_meb10180/eodata/train_preprocessed_all.json\n",
      "train_preprocessed_ds_path: hdfs://rpc.namenode.service.consul:8020/Projects/demo_ml_meb10180/eodata/train_preprocessed.json\n",
      "test_preprocessed_ds_path: hdfs://rpc.namenode.service.consul:8020/Projects/demo_ml_meb10180/eodata/test_preprocessed.json"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = 'eodata'\n",
    "train_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER,'train.json')\n",
    "train_preprocessed_all_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'train_preprocessed_all.json')\n",
    "train_preprocessed_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'train_preprocessed.json')\n",
    "test_preprocessed_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'test_preprocessed.json')\n",
    "\n",
    "print(\"train_ds_path:\", train_ds_path)\n",
    "print(\"train_preprocessed_all_ds_path:\", train_preprocessed_all_ds_path)\n",
    "print(\"train_preprocessed_ds_path:\", train_preprocessed_ds_path)\n",
    "print(\"test_preprocessed_ds_path:\", test_preprocessed_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read raw train with spark and insert into feature store\n",
    "train_preprocessed_all_df = spark.read.format('json').load(train_preprocessed_all_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- band_1: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- band_2: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- band_avg: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- inc_angle: string (nullable = true)\n",
      " |-- is_iceberg: long (nullable = true)"
     ]
    }
   ],
   "source": [
    "train_preprocessed_all_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------+---------+----------+\n",
      "|              band_1|              band_2|            band_avg|      id|inc_angle|is_iceberg|\n",
      "+--------------------+--------------------+--------------------+--------+---------+----------+\n",
      "|[-27.878361, -27....|[-27.154118, -29....|[-27.5162395, -28...|dfd5f913|  43.9239|         0|\n",
      "|[-12.242375, -14....|[-31.506321, -27....|[-21.874348, -21....|e25388fd|  38.1562|         0|\n",
      "|[-24.603676, -24....|[-24.870956, -24....|[-24.737316, -24....|58b2aaa0|  45.2859|         1|\n",
      "|[-22.454607, -23....|[-27.889421, -27....|[-25.172014, -25....|4cfc3a18|  43.8306|         0|\n",
      "|[-26.006956, -23....|[-27.206915, -30....|[-26.6069355, -26...|271f93f4|  35.6256|         0|\n",
      "+--------------------+--------------------+--------------------+--------+---------+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "train_preprocessed_all_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "conn = hsfs.connection()\n",
    "fs = conn.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create feature expectations with validation rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expectation.rules[0].to_dict(){'name': 'HAS_DATATYPE', 'level': 'ERROR', 'min': None, 'max': 0, 'value': None, 'pattern': None, 'acceptedType': 'Null', 'legalValues': None}\n",
      "ExpectationsApi.expectation.to_dict(){'name': 'icebergs_id', 'description': 'validate inc_angle feature values', 'features': ['id'], 'rules': [<hsfs.rule.Rule object at 0x7f32b971d610>]}\n",
      "ExpectationsApi.expectation.rules[0].to_dict(){'name': 'HAS_DATATYPE', 'level': 'ERROR', 'min': None, 'max': 0, 'value': None, 'pattern': None, 'acceptedType': 'Null', 'legalValues': None}\n",
      "ExpectationsApi.expectation.payload{\"name\": \"icebergs_id\", \"description\": \"validate inc_angle feature values\", \"features\": [\"id\"], \"rules\": [{\"name\": \"HAS_DATATYPE\", \"level\": \"ERROR\", \"min\": null, \"max\": 0, \"value\": null, \"pattern\": null, \"acceptedType\": \"Null\", \"legalValues\": null}]}\n",
      "expectation.rules[0].to_dict(){'name': 'HAS_DATATYPE', 'level': 'ERROR', 'min': 1, 'max': None, 'value': None, 'pattern': None, 'acceptedType': 'Integral', 'legalValues': None}\n",
      "ExpectationsApi.expectation.to_dict(){'name': 'is_iceberg', 'description': 'validate is_iceberg label values', 'features': ['is_iceberg'], 'rules': [<hsfs.rule.Rule object at 0x7f32b9e3b110>, <hsfs.rule.Rule object at 0x7f32b9714250>, <hsfs.rule.Rule object at 0x7f32b9718790>]}\n",
      "ExpectationsApi.expectation.rules[0].to_dict(){'name': 'HAS_DATATYPE', 'level': 'ERROR', 'min': 1, 'max': None, 'value': None, 'pattern': None, 'acceptedType': 'Integral', 'legalValues': None}\n",
      "ExpectationsApi.expectation.payload{\"name\": \"is_iceberg\", \"description\": \"validate is_iceberg label values\", \"features\": [\"is_iceberg\"], \"rules\": [{\"name\": \"HAS_DATATYPE\", \"level\": \"ERROR\", \"min\": 1, \"max\": null, \"value\": null, \"pattern\": null, \"acceptedType\": \"Integral\", \"legalValues\": null}, {\"name\": \"HAS_MAX\", \"level\": \"ERROR\", \"min\": 1, \"max\": 1, \"value\": null, \"pattern\": null, \"acceptedType\": null, \"legalValues\": null}, {\"name\": \"HAS_MIN\", \"level\": \"ERROR\", \"min\": 0, \"max\": 0, \"value\": null, \"pattern\": null, \"acceptedType\": null, \"legalValues\": null}]}"
     ]
    }
   ],
   "source": [
    "expectation_id = fs.create_expectation(\"icebergs_id\",  \n",
    "                                       description=\"validate inc_angle feature values\",\n",
    "                                       features=[\"id\"], \n",
    "                                       rules=[Rule(name=\"HAS_DATATYPE\", level=\"ERROR\",accepted_type=\"Null\", max=0)])\n",
    "expectation_id.save()\n",
    "\n",
    "expectation_label = fs.create_expectation(\"is_iceberg\",\n",
    "                                         features=[\"is_iceberg\"], \n",
    "                                         description=\"validate is_iceberg label values\",\n",
    "                                         rules=[Rule(name=\"HAS_DATATYPE\", level=\"ERROR\", accepted_type=\"Integral\", min=1), \n",
    "                                                Rule(name=\"HAS_MAX\", level=\"ERROR\", min=1, max=1), \n",
    "                                                Rule(name=\"HAS_MIN\", level=\"ERROR\", min=0, max=0)])\n",
    "\n",
    "expectation_label.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and save features to the Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "icebergs_fg = fs.create_feature_group(\n",
    "    \"iceberg\",\n",
    "    time_travel_format=None,\n",
    "    statistics_config=hsfs.statistics_config.StatisticsConfig(enabled=False, correlations=False, histograms=False, columns=[]),\n",
    "    expectations=[expectation_id, expectation_label],\n",
    "    validation_type=\"STRICT\",\n",
    "    description=\"Training dataset in Feature Store for iceberg classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7f32b9714350>\n",
      "VersionWarning: No version provided for creating feature group `iceberg`, incremented version to `2`."
     ]
    }
   ],
   "source": [
    "icebergs_fg.save(train_preprocessed_all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retreving validation results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"validationId\": 1025,\n",
      "  \"validationTime\": 1623662197358,\n",
      "  \"expectationResults\": [\n",
      "    {\n",
      "      \"expectation\": {\n",
      "        \"features\": [\n",
      "          \"is_iceberg\"\n",
      "        ],\n",
      "        \"rules\": [\n",
      "          {\n",
      "            \"level\": \"ERROR\",\n",
      "            \"min\": 1.0,\n",
      "            \"name\": \"HAS_DATATYPE\"\n",
      "          },\n",
      "          {\n",
      "            \"level\": \"ERROR\",\n",
      "            \"max\": 1.0,\n",
      "            \"min\": 1.0,\n",
      "            \"name\": \"HAS_MAX\"\n",
      "          },\n",
      "          {\n",
      "            \"level\": \"ERROR\",\n",
      "            \"max\": 0.0,\n",
      "            \"min\": 0.0,\n",
      "            \"name\": \"HAS_MIN\"\n",
      "          }\n",
      "        ],\n",
      "        \"description\": \"validate is_iceberg label values\",\n",
      "        \"name\": \"is_iceberg\"\n",
      "      },\n",
      "      \"results\": [\n",
      "        {\n",
      "          \"features\": [\n",
      "            \"is_iceberg\"\n",
      "          ],\n",
      "          \"message\": \"Success\",\n",
      "          \"rule\": {\n",
      "            \"level\": \"ERROR\",\n",
      "            \"min\": 1.0,\n",
      "            \"name\": \"HAS_DATATYPE\"\n",
      "          },\n",
      "          \"status\": \"SUCCESS\",\n",
      "          \"value\": \"Distribution(Map(Boolean -> DistributionValue(0,0.0), Fractional -> DistributionValue(0,0.0), Integral -> DistributionValue(1604,1.0), Unknown -> DistributionValue(0,0.0), String -> DistributionValue(0,0.0)),5)\"\n",
      "        },\n",
      "        {\n",
      "          \"features\": [\n",
      "            \"is_iceberg\"\n",
      "          ],\n",
      "          \"message\": \"Success\",\n",
      "          \"rule\": {\n",
      "            \"level\": \"ERROR\",\n",
      "            \"max\": 1.0,\n",
      "            \"min\": 1.0,\n",
      "            \"name\": \"HAS_MAX\"\n",
      "          },\n",
      "          \"status\": \"SUCCESS\",\n",
      "          \"value\": \"1.0\"\n",
      "        },\n",
      "        {\n",
      "          \"features\": [\n",
      "            \"is_iceberg\"\n",
      "          ],\n",
      "          \"message\": \"Success\",\n",
      "          \"rule\": {\n",
      "            \"level\": \"ERROR\",\n",
      "            \"max\": 0.0,\n",
      "            \"min\": 0.0,\n",
      "            \"name\": \"HAS_MIN\"\n",
      "          },\n",
      "          \"status\": \"SUCCESS\",\n",
      "          \"value\": \"0.0\"\n",
      "        }\n",
      "      ],\n",
      "      \"status\": \"SUCCESS\"\n",
      "    },\n",
      "    {\n",
      "      \"expectation\": {\n",
      "        \"features\": [\n",
      "          \"id\"\n",
      "        ],\n",
      "        \"rules\": [\n",
      "          {\n",
      "            \"level\": \"ERROR\",\n",
      "            \"max\": 0.0,\n",
      "            \"name\": \"HAS_DATATYPE\"\n",
      "          }\n",
      "        ],\n",
      "        \"description\": \"validate inc_angle feature values\",\n",
      "        \"name\": \"icebergs_id\"\n",
      "      },\n",
      "      \"results\": [\n",
      "        {\n",
      "          \"features\": [\n",
      "            \"id\"\n",
      "          ],\n",
      "          \"message\": \"Success\",\n",
      "          \"rule\": {\n",
      "            \"level\": \"ERROR\",\n",
      "            \"max\": 0.0,\n",
      "            \"name\": \"HAS_DATATYPE\"\n",
      "          },\n",
      "          \"status\": \"SUCCESS\",\n",
      "          \"value\": \"Distribution(Map(Boolean -> DistributionValue(0,0.0), Fractional -> DistributionValue(0,0.0), Integral -> DistributionValue(40,0.02493765586034913), Unknown -> DistributionValue(0,0.0), String -> DistributionValue(1564,0.9750623441396509)),5)\"\n",
      "        }\n",
      "      ],\n",
      "      \"status\": \"SUCCESS\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "[None]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "[print(json.dumps(validation.to_dict(), indent=2)) for validation in icebergs_fg.get_validations()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split\n",
    "Now that preprocessing is done, let's split the feature data into training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAND_SEED = 42\n",
    "TRAIN_SIZE = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604"
     ]
    }
   ],
   "source": [
    "icebergs_fg.read().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read feature group data, split into train/test and export in tfrecords\n",
    "icebergs_train_df, icebergs_test_df = icebergs_fg.read().randomSplit([TRAIN_SIZE, 1-TRAIN_SIZE], RAND_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset contains 1263 records"
     ]
    }
   ],
   "source": [
    "print(\"Training dataset contains {} records\".format(icebergs_train_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dataset contains 341 records"
     ]
    }
   ],
   "source": [
    "print(\"Testing dataset contains {} records\".format(icebergs_test_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VersionWarning: No version provided for creating training dataset `train_tfrecords_iceberg_classification_dataset`, incremented version to `2`."
     ]
    }
   ],
   "source": [
    "# create a training dataset of TFRecord\n",
    "icebergs_train_td = fs.create_training_dataset(\n",
    "    \"train_tfrecords_iceberg_classification_dataset\",\n",
    "    statistics_config=hsfs.statistics_config.StatisticsConfig(enabled=False, correlations=False, histograms=False, columns=[]),\n",
    "    data_format = \"tfrecords\"\n",
    ").save(icebergs_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VersionWarning: No version provided for creating training dataset `test_tfrecords_iceberg_classification_dataset`, incremented version to `2`."
     ]
    }
   ],
   "source": [
    "# create a training dataset of TFRecord\n",
    "icebergs_test_td = fs.create_training_dataset(\n",
    "    \"test_tfrecords_iceberg_classification_dataset\",\n",
    "    statistics_config=hsfs.statistics_config.StatisticsConfig(enabled=False, correlations=False, histograms=False, columns=[]),\n",
    "    data_format = \"tfrecords\"\n",
    ").save(icebergs_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}