{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Classification Step 1: Preprocessing the data\n",
    "\n",
    "This notebook performs the data preprocessing step in the end-to-end pipeline.\n",
    "\n",
    "This notebook will perform the following operations:\n",
    "- Take the 'train.json' as input file\n",
    "- Do some preprocessing and data engineering\n",
    "- Save the preprocessed dataset in a Hopsworks dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>61</td><td>application_1623657485747_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://maggydemo22-master.internal.cloudapp.net:8089/proxy/application_1623657485747_0001/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://maggydemo22-worker-1.internal.cloudapp.net:8044/node/containerlogs/container_e10_1623657485747_0001_01_000001/demo_ml_meb10180__meb10180\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "-----------------------------------------------\n",
      "This notebook is tested with:\n",
      "  - Hopsworks 2.2.0.1.\n",
      "  - Spark 2.4.3.2."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import hops\n",
    "from hops import hdfs\n",
    "from hops import pandas_helper as pd\n",
    "\n",
    "# SparkSession available as 'spark'\n",
    "print(\n",
    "    f\"-----------------------------------------------\\n\" \\\n",
    "    f\"This notebook is tested with:\\n\" \\\n",
    "    f\"  - Hopsworks {hops.__version__}.\\n\" \\\n",
    "    f\"  - Spark {spark.version}.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define relevant paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds_path: hdfs://rpc.namenode.service.consul:8020/Projects/demo_ml_meb10180/eodata/train.json\n",
      "train_preprocessed_all_ds_path: hdfs://rpc.namenode.service.consul:8020/Projects/demo_ml_meb10180/eodata/train_preprocessed_all.json"
     ]
    }
   ],
   "source": [
    "# 'DATA_FOLDER' refers to the folder name under the Data Sets page in hopsworks UI.\n",
    "# In this case, the training dataset is stored in the 'eodata' folder.\n",
    "DATA_FOLDER = 'eodata'\n",
    "\n",
    "# Get the paths to read the original dataset and save the preprocessed dataset.\n",
    "train_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'train.json')\n",
    "train_preprocessed_all_ds_path = os.path.join(hdfs.project_path(), DATA_FOLDER, 'train_preprocessed_all.json')\n",
    "\n",
    "print(f\"train_ds_path: {train_ds_path}\")\n",
    "print(f\"train_preprocessed_all_ds_path: {train_preprocessed_all_ds_path}\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the raw data\n",
    "\n",
    "The data \\[[iceberg dataset](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge/data)\\] (train.json) is presented in json format. The files consist of a list of images, and for each image, you can find the following fields:\n",
    "\n",
    "- ``id``: the id of the image\n",
    "- ``band_1``, ``band_2``: the flattened image data. Each band has 75x75 pixel values in the list, so the list has 5625 elements. Note that these values are not the normal non-negative integers in image files since they have physical meanings - these are float numbers with unit being dB. Band 1 and Band 2 are signals characterized by radar backscatter produced from different polarizations at a particular incidence angle. The polarizations correspond to HH (transmit/receive horizontally) and HV (transmit horizontally and receive vertically). More background on the satellite imagery can be found here.\n",
    "- ``inc_angle``: the incidence angle of which the image was taken. Note that this field has missing data marked as \"na\", and those images with \"na\" incidence angles are all in the training data to prevent leakage.\n",
    "- ``is_iceberg``: the target variable, set to 1 if it is an iceberg, and 0 if it is a ship. This field only exists in train.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw data to pandas dataframe using the pandas function provided in hopsworks.\n",
    "# Note that the read_json function provided by hopsworks is needed since we are reading over hdfs.\n",
    "raw_train_df = pd.read_json(train_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1604 entries, 0 to 1603\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          1604 non-null   object\n",
      " 1   band_1      1604 non-null   object\n",
      " 2   band_2      1604 non-null   object\n",
      " 3   inc_angle   1604 non-null   object\n",
      " 4   is_iceberg  1604 non-null   int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 62.8+ KB"
     ]
    }
   ],
   "source": [
    "raw_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id  ... is_iceberg\n",
      "0     dfd5f913  ...          0\n",
      "1     e25388fd  ...          0\n",
      "2     58b2aaa0  ...          1\n",
      "3     4cfc3a18  ...          0\n",
      "4     271f93f4  ...          0\n",
      "...        ...  ...        ...\n",
      "1599  04e11240  ...          0\n",
      "1600  c7d6f6f8  ...          0\n",
      "1601  bba1a0f1  ...          0\n",
      "1602  7f66bb44  ...          0\n",
      "1603  9d8f326c  ...          0\n",
      "\n",
      "[1604 rows x 5 columns]"
     ]
    }
   ],
   "source": [
    "raw_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new feature band_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for taking list average\n",
    "def list_avg(row):\n",
    "    return [sum(x)/2 for x in zip(row['band_1'], row['band_2'])]\n",
    "\n",
    "# Construct a new feature called 'band_avg' by taking element-wise average from 'band_1' and 'band_2'\n",
    "raw_train_df['band_avg'] = raw_train_df.apply(lambda row: list_avg(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id  ...                                           band_avg\n",
      "0     dfd5f913  ...  [-27.516239499999998, -28.346024, -29.84960749...\n",
      "1     e25388fd  ...  [-21.874347999999998, -21.4524295, -20.7830205...\n",
      "2     58b2aaa0  ...  [-24.737316, -24.348173, -22.762496, -21.28190...\n",
      "3     4cfc3a18  ...  [-25.172013999999997, -25.301306500000003, -25...\n",
      "4     271f93f4  ...  [-26.6069355, -26.712035999999998, -26.7120359...\n",
      "...        ...  ...                                                ...\n",
      "1599  04e11240  ...  [-29.4237985, -29.105365, -26.472991999999998,...\n",
      "1600  c7d6f6f8  ...  [-27.437631500000002, -27.400965, -27.76694599...\n",
      "1601  bba1a0f1  ...  [-21.723625, -23.7647725, -23.9906165, -22.930...\n",
      "1602  7f66bb44  ...  [-24.262994499999998, -23.944199, -24.2661145,...\n",
      "1603  9d8f326c  ...  [-22.1770305, -22.817203499999998, -23.9654685...\n",
      "\n",
      "[1604 rows x 6 columns]"
     ]
    }
   ],
   "source": [
    "raw_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started copying local path train_preprocessed_all.json to hdfs path hdfs://rpc.namenode.service.consul:8020/Projects/demo_ml_meb10180/eodata/train_preprocessed_all.json\n",
      "\n",
      "Finished copying"
     ]
    }
   ],
   "source": [
    "# Save raw train df in dataset.\n",
    "raw_train_df.to_json(path_or_buf='train_preprocessed_all.json', orient='records')\n",
    "\n",
    "# Note that one needs to be owner of 'DATA_FOLDER' in order to write the file to the folder.\n",
    "# Otherwise, permission error might occur.\n",
    "hdfs.copy_to_hdfs('train_preprocessed_all.json', DATA_FOLDER , overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Step 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}